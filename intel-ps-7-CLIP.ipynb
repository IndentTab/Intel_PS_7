{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom datasets import load_dataset\nfrom transformers import CLIPProcessor, CLIPModel\nfrom tqdm.auto import tqdm\n\n# Load CLIP model and processor\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\nprocessor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n# Load dataset in streaming mode to avoid long download times\ndataset = load_dataset(\"primecai/dsd_data\", split=\"train\", streaming=True)\n\n# Limit the number of samples to process (avoid Kaggle crash)\nmax_samples = 50000\ndata = []\ncount = 0\n\nfor item in tqdm(dataset, total=max_samples):\n    if count >= max_samples:\n        break  # Stop after processing max_samples\n\n    try:\n        # Extract image and text\n        image = item[\"conditioning\"]\n        text = item[\"caption\"]\n\n        # Process image\n        image_inputs = processor(images=image, return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            image_embedding = model.get_image_features(**image_inputs).cpu().numpy().flatten()\n\n        # Process text\n        text_inputs = processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True).to(device)\n        with torch.no_grad():\n            text_embedding = model.get_text_features(**text_inputs).cpu().numpy().flatten()\n\n        # Append results\n        data.append({\n            \"Text Description\": text,\n            \"Image Embedding\": image_embedding.tolist(),\n            \"Text Embedding\": text_embedding.tolist()\n        })\n\n        count += 1  # Increase count only when successfully processed\n\n    except Exception as e:\n        print(f\"Skipping sample due to error: {e}\")\n\n# Convert to DataFrame\ndf = pd.DataFrame(data)\n\n# Save to CSV\noutput_path = \"/kaggle/working/dsd_embeddings.csv\"\ndf.to_csv(output_path, index=False)\n\nprint(f\"Embeddings saved to {output_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-17T19:08:42.043071Z","iopub.execute_input":"2025-03-17T19:08:42.043395Z","iopub.status.idle":"2025-03-17T19:46:46.156383Z","shell.execute_reply.started":"2025-03-17T19:08:42.043367Z","shell.execute_reply":"2025-03-17T19:46:46.155432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import CLIPTokenizer, CLIPModel\n\n# Load CLIP model and tokenizer\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\ntokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n\ndef get_text_embedding(text):\n    \"\"\"Takes a text query and returns its CLIP embedding as a NumPy array.\"\"\"\n    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n\n    with torch.no_grad():\n        text_embedding = model.get_text_features(**inputs).cpu().numpy().flatten()\n\n    return text_embedding\n\n# Example usage\nquery = \"A beautiful sunset over the mountains.\"\nembedding = get_text_embedding(query)\n\nprint(embedding[:5]) #prints only first 5 embeddings ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-22T12:18:38.070062Z","iopub.execute_input":"2025-03-22T12:18:38.070642Z","iopub.status.idle":"2025-03-22T12:18:39.280220Z","shell.execute_reply.started":"2025-03-22T12:18:38.070591Z","shell.execute_reply":"2025-03-22T12:18:39.278169Z"}},"outputs":[],"execution_count":null}]}